

# ğŸ¥ NurseCare Robot

An **interactive assistive robot** that uses **HEBI actuators**, **MediaPipe pose detection**, and **speech/sound feedback** to recognize human gestures and respond with **lifelike robot actions**.

---

## âœ¨ Features

* ğŸ‘‹ **Wave detection** â†’ Robot waves back using actuator
* ğŸ™‡ **Bow detection** â†’ Robot performs a smooth bow
* ğŸ‘ **Clap detection** â†’ Detects hand clap, plays sound + small motion
* ğŸª‘ **Sit detection** â†’ Detects seated posture
* ğŸš¨ **Fall detection** â†’ Triggers alarm + speech warning
* ğŸ”Š **Speech feedback** (`pyttsx3`)
* ğŸµ **Sound effects** (`playsound`)
* ğŸŒ€ **Idle sway** for natural presence
* ğŸ“· OpenCV HUD with gesture labels

---

## âš™ï¸ Installation

```bash
pip install opencv-python mediapipe hebi-py pyttsx3 playsound==1.2.2
```

---

## ğŸš€ Usage

1. Connect your HEBI actuators.
2. Edit `nurse_robot.py` with your HEBI family/module names:

   ```python
   family_name = "Enggar1"
   module_names = ["Rear_right", "Rear_left"]
   ```
3. Run:

   ```bash
   python nurse_robot.py
   ```

---

## ğŸ“· System Flow

```
Camera â”€â”€â–¶ MediaPipe Pose â”€â”€â–¶ Gesture Recognition
                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                â–¼
   Speech + Sound   HEBI Actuator Motion
```

---

## ğŸ“œ License

MIT License

---

---

# ğŸ™ï¸ Voice Chatbot (LM Studio + Whisper + ElevenLabs)

A **Streamlit web app** for voice-enabled AI chat, combining:

* **LM Studio** for local LLM responses
* **Whisper (faster-whisper)** for speech-to-text
* **ElevenLabs** for natural text-to-speech

---

## âœ¨ Features

* ğŸ¤ Record voice or upload `.wav` files
* ğŸ§  Query local LLM (LM Studio server)
* ğŸ—£ï¸ Transcribe audio with Whisper
* ğŸ”Š Generate spoken replies with ElevenLabs TTS
* âš™ï¸ Customizable via sidebar (model, voice, output format)

---

## âš™ï¸ Installation

```bash
pip install streamlit numpy python-dotenv openai faster-whisper soundfile requests streamlit-mic-recorder
```

---

## ğŸ”‘ Environment Setup

Create a `.env` file:

```env
LLM_BASE_URL=http://localhost:1234/v1
LLM_MODEL=openai/gpt-oss-20b
OPENAI_API_KEY=lm-studio
WHISPER_MODEL=small
TTS_BACKEND=elevenlabs
ELEVEN_API_KEY=your-elevenlabs-key
ELEVEN_VOICE_ID=JBFqnCBsd6RMkjVDRZzb
ELEVEN_OUTPUT_FORMAT=mp3_44100_128
```

---

## ğŸš€ Usage

1. Start LM Studio (Developer â†’ Start Server).
2. Run:

   ```bash
   streamlit run app.py
   ```
3. Interact:

   * Record your question ğŸ¤
   * Upload audio ğŸµ
   * Or type a message âŒ¨ï¸
   * Hear spoken reply ğŸ”Š

---

## ğŸ“· Workflow

```
Mic / Audio â”€â”€â–¶ Whisper STT â”€â”€â–¶ LM Studio LLM â”€â”€â–¶ ElevenLabs TTS â”€â”€â–¶ Spoken Reply
```

---

## ğŸ“œ License

MIT License

---

ğŸ‘‰ This way you can place:

* `nurse-robot/README.md`
* `voice-chatbot/README.md`


